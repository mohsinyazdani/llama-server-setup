# üöÄ LLama Server - Complete Auto-Setup Solution

<p align="center">
  <img src="assets/llama-service-logo.png" alt="Llama Service Logo" width="320" />
</p>

<p align="center">
  <a href="#-supported-platforms"><img alt="Platforms" src="https://img.shields.io/badge/platforms-Windows%20|%20Linux%20|%20macOS-1f6feb" /></a>
  <a href="#-docker-usage"><img alt="Docker" src="https://img.shields.io/badge/docker-ready-0db7ed" /></a>
  <a href="#-testing--api-usage"><img alt="API" src="https://img.shields.io/badge/API-OpenAI%20compatible-28a745" /></a>
  <a href="#-manual-setup-if-auto-setup-fails"><img alt="Fallback" src="https://img.shields.io/badge/setup-auto%20%2B%20manual-6f42c1" /></a>
</p>

**One-click setup for LLama server on any platform!**

 LLama server implementation with automatic setup, dependency management, and cross-platform support. 

---

## üìö Table of Contents

- [‚ö° Super Quick Start](#-super-quick-start)
- [üîß Manual setup if auto-setup fails](#-manual-setup-if-auto-setup-fails)
- [üñ•Ô∏è Supported Platforms](#-supported-platforms)
- [üìã What Auto-Setup Does](#-what-auto-setup-does)
- [üìÅ Directory Structure After Setup](#-directory-structure-after-setup)
- [üéØ Usage After Setup](#-usage-after-setup)
- [üß™ Testing & API Usage](#-testing--api-usage)
- [üîß Features](#-features)
- [üõ†Ô∏è Advanced Configuration](#-advanced-configuration)
- [üÜò Troubleshooting](#-troubleshooting)
- [üîÑ Updates & Maintenance](#-updates--maintenance)
- [üê≥ Docker Usage](#-docker-usage)
- [üá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è](#-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç-—Ä—É—Å—Å–∫–∏–π)

## ‚ö° Super Quick Start

### All Platforms
1) Download a GGUF model to `models/` (recommended Phi-4-mini):
   - Source: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
   - File name: `models/phi-4-mini-instruct-q4_k_m.gguf`

2) Run the auto-setup script:
```bash
python main.py
```

3) Start the server using the generated script (`start.bat` on Windows or `./start.sh` on Linux/macOS).

The auto-setup handles the server binary and dependencies automatically:
- ‚úÖ Auto-installs its own dependencies (requests)
- ‚úÖ Detects your platform and downloads correct files
- ‚úÖ Creates platform-specific startup scripts

Note: If auto-setup fails to fetch the correct server for your OS/arch, see the manual setup steps below.

## üîß Manual setup if auto-setup fails

If the auto-setup script can‚Äôt fetch the correct llama server binary for your OS/arch, follow these steps:

1) Download the correct llama.cpp server build from: https://github.com/ggml-org/llama.cpp/releases
- Windows x64: grab the zip containing `llama-server.exe` from the latest llama.cpp release
- Linux x64/ARM64: grab the zip containing `llama-server`
- macOS Intel/ARM64: grab the zip containing `llama-server`

2) Extract the archive locally and copy files into the project root
- Place the server binary as:
  - Windows: `server.exe`
-  Linux/macOS: `server`
- Copy all required libraries next to it (DLLs on Windows, `.so` on Linux, `.dylib` on macOS)

3) Create the models directory and add a GGUF model
```bash
mkdir -p models
# Download the model:
# https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
# Place as: models/phi-4-mini-instruct-q4_k_m.gguf
```

4) Create a startup script
- Windows `start.bat`:
```bat
@echo off
set MODEL_PATH=models\phi-4-mini-instruct-q4_k_m.gguf
set HOST=0.0.0.0
set PORT=8080
set CONTEXT_SIZE=4096
set THREADS=8
set GPU_LAYERS=0

server.exe ^
  -m "%MODEL_PATH%" ^
  --host "%HOST%" ^
  --port "%PORT%" ^
  -c "%CONTEXT_SIZE%" ^
  -t "%THREADS%" ^
  --n-gpu-layers "%GPU_LAYERS%"
```

- Linux/macOS `start.sh`:
```bash
#!/usr/bin/env bash
set -euo pipefail
MODEL_PATH="models/phi-4-mini-instruct-q4_k_m.gguf"
HOST="0.0.0.0"
PORT="8080"
CONTEXT_SIZE="4096"
THREADS="8"
GPU_LAYERS="0"

chmod +x ./server
./server \
  -m "$MODEL_PATH" \
  --host "$HOST" \
  --port "$PORT" \
  -c "$CONTEXT_SIZE" \
  -t "$THREADS" \
  --n-gpu-layers "$GPU_LAYERS"
```

5) Start and test
```bash
# Windows
start.bat

# Linux/macOS
./start.sh

# Health
curl http://localhost:8080/health
```

## üñ•Ô∏è Supported Platforms

| Platform | Architecture | Status |
|----------|-------------|---------|
| Windows | x64 | ‚úÖ Fully Supported |
| Linux | x64 | ‚úÖ Fully Supported |
| Linux | ARM64 | ‚úÖ Fully Supported |
| macOS | Intel (x64) | ‚úÖ Fully Supported |
| macOS | Apple Silicon (ARM64) | ‚úÖ Fully Supported |

## üìã What Auto-Setup Does

1. ‚úÖ **Detects your platform** (Windows/Linux/macOS, x64/ARM64)
2. ‚úÖ **Downloads server executable** (correct version for your system)
3. ‚úÖ **Downloads all dependencies** (DLLs, shared libraries)
4. ‚úÖ **Creates startup scripts** (platform-optimized)
5. ‚úÖ **Creates test utilities** (API testing script)
6. ‚úÖ **Sets up directory structure** (models folder, documentation)

## üìÅ Directory Structure

```
your-project/
‚îú‚îÄ‚îÄ main.py                               # Entry point script
‚îú‚îÄ‚îÄ setup.py                              # Core setup class
‚îú‚îÄ‚îÄ platform_detector.py                  # Platform detection module
‚îú‚îÄ‚îÄ downloader.py                         # Download utilities
‚îú‚îÄ‚îÄ extractor.py                          # File extraction utilities
‚îú‚îÄ‚îÄ file_creator.py                       # Startup/test scripts creator
‚îú‚îÄ‚îÄ auto_setup_llama.py                   # Legacy setup script
‚îú‚îÄ‚îÄ models/                               # Model files directory
‚îÇ   ‚îú‚îÄ‚îÄ phi-4-mini-instruct-q4_k_m.gguf  # AI model (2.2GB)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        # Model download guide
‚îú‚îÄ‚îÄ server.exe (or server)               # Main executable (after setup)
‚îú‚îÄ‚îÄ *.dll (or *.so/*.dylib)             # Required libraries (after setup)
‚îú‚îÄ‚îÄ start.bat (or start.sh)             # Startup script (after setup)
‚îú‚îÄ‚îÄ test_api.py                          # API test script (after setup)
‚îú‚îÄ‚îÄ Dockerfile                            # Docker configuration
‚îî‚îÄ‚îÄ README.md                            # This file
```

## üéØ Usage After Setup

Refer to Super Quick Start steps (download GGUF ‚Üí run auto-setup ‚Üí start script).

- Optional: Run `python test_api.py` to verify endpoints
- Open the Web UI at http://localhost:8080

## üéâ You're Done!

Your LLama server is running at `http://localhost:8080` with:
- ‚úÖ OpenAI-compatible API endpoints
- ‚úÖ Web interface for testing
- ‚úÖ All dependencies properly configured

## üìã What Gets Downloaded

### Windows
- `server.exe` - LLama server executable
- `ggml-base.dll` - Core GGML library
- `ggml-cpu-*.dll` - CPU optimization libraries
- `ggml.dll` - GGML interface
- `llama.dll` - LLama library
- `libcurl-x64.dll` - HTTP client
- `libomp140.x86_64.dll` - OpenMP runtime
- Additional support DLLs

### Linux/macOS
- `server` - LLama server executable
- `*.so` / `*.dylib` - Shared libraries
- All required dependencies

 

## üß™ Testing & API Usage

### Test Everything Works
```bash
# Run the comprehensive test script
python test_api.py

# Or check manually
curl http://localhost:8080/health
```

<details>
<summary><strong>API Examples (expand)</strong></summary>

#### Simple Completion
```bash
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is artificial intelligence?",
    "n_predict": 100,
    "temperature": 0.7
  }'
```

#### OpenAI-Compatible Chat
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 50
  }'
```

Note: The "model" field is required for OpenAI compatibility but the actual model used is whatever GGUF file you loaded (e.g., `phi-4-mini-instruct-q4_k_m.gguf`).

#### Health Check
```bash
curl http://localhost:8080/health
```

#### Server Info
```bash
curl http://localhost:8080/v1/models
```

</details>



## üîß Features

### Smart Platform Detection
- Automatically detects Windows/Linux/macOS
- Identifies x64/ARM64 architecture
- Downloads correct binaries for your system

### Complete Dependency Management
- Downloads ALL required files
- No missing DLL/library issues
- Ready-to-run setup

### User-Friendly Scripts
- Platform-appropriate startup scripts
- Comprehensive test scripts
- Clear error messages and guidance

### Progress Tracking
- Download progress bars
- Step-by-step status updates
- Clear success/failure indicators

### Production Ready
- **High performance**: Native C++ llama.cpp implementation
- **OpenAI-compatible API**: Drop-in replacement for OpenAI endpoints
- **Low memory footprint**: Minimal resource usage
- **Fast startup**: No model loading delays

## üõ†Ô∏è Advanced Configuration

### Custom Configuration
Edit the generated startup script to modify:
- Model path
- Server host/port
- Context size
- Thread count
- GPU layers

### Multiple Models
- Place multiple GGUF files in `models/`
- Update startup script MODEL_PATH variable
- Switch between models as needed

### Server Configuration Options

| Option | Description | Default |
|--------|-------------|---------|
| `-m, --model` | Path to model file | Required |
| `--host` | IP address to bind to | 0.0.0.0 |
| `--port` | Port to listen on | 8080 |
| `-c, --ctx-size` | Context window size | 4096 |
| `-t, --threads` | Number of threads | 8 |
| `--n-gpu-layers` | GPU layers to offload | 0 |
| `--log-disable` | Disable logging | false |

## üÜò Troubleshooting

### Setup Problems
- **Python not found**: Install Python 3.7+ from python.org, ensure "Add to PATH" is checked
- **Download fails**: Check internet connection, try running setup again (auto-resumes)
- **Requests install fails**: Manually run `pip install requests` then retry setup

### Server Problems  
- **Model not found**: Download a GGUF model to `models/` folder
- **Port busy**: Change PORT in startup script
- **Out of memory**: Use smaller model or reduce context size
- **Missing DLLs**: Re-run auto-setup to download all dependencies

### Quick Diagnostics
```bash
# Test if everything works
python test_api.py

# Check server health
curl http://localhost:8080/health

# Verify model exists
dir models\*.gguf    # Windows
ls models/*.gguf     # Linux/macOS

# Re-run setup if something went wrong
python main.py
```

## üîÑ Updates & Maintenance

### Update to Latest Version
1. Run the setup script again: `python main.py`
2. Choose 'y' when asked to overwrite existing files
3. Existing models and custom configs are preserved

### Clean Installation
1. Delete `server.exe` and `*.dll` files
2. Run setup script for fresh installation
3. Models in `models/` directory are kept

## üöÄ What's Next?

- **Integrate**: Use OpenAI-compatible API in your apps
- **Customize**: Edit startup scripts for your needs  
- **Scale**: Deploy to cloud or run multiple instances
- **Explore**: Try different models from Hugging Face

## üìö Resources & Support

- **Model Downloads**: Check `models/README.md` for recommended models and download links
- **Issues**: Report problems via GitHub Issues
- **Testing**: Use `python test_api.py` for comprehensive diagnostics
- **API Documentation**: OpenAI-compatible endpoints at `/v1/chat/completions`
- **Configuration**: Edit startup scripts for custom settings

## üê≥ Docker Usage

<details>
<summary><strong>Docker quick start and configuration (expand)</strong></summary>

### Quick Start with Docker

1. **Build the image:**
```bash
docker build -t llama-server .
```

2. **Download a model** to your local `models/` directory first:
```bash
# Example: Download Phi-4-mini model (Q4_K_M) from Hugging Face
# Place phi-4-mini-instruct-q4_k_m.gguf in ./models/
# Source: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
```

3. **Run the container:**
```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  llama-server
```

### Docker Environment Variables

Customize the server by setting environment variables:

```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  -e MODEL_PATH="models/your-model.gguf" \
  -e CONTEXT_SIZE="8192" \
  -e THREADS="16" \
  llama-server
```

### Available Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `MODEL_PATH` | Path to GGUF model file | `models/phi-4-mini-instruct-q4_k_m.gguf` |
| `HOST` | Host to bind to | `0.0.0.0` |
| `PORT` | Port to listen on | `8080` |
| `CONTEXT_SIZE` | Context window size | `4096` |
| `THREADS` | Number of threads | `8` |
| `GPU_LAYERS` | GPU layers to offload | `0` |

### Docker Compose (Optional)

Create a `docker-compose.yml`:

```yaml
version: '3.8'
services:
  llama-server:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_PATH=models/phi-4-mini-instruct-q4_k_m.gguf
      - CONTEXT_SIZE=4096
      - THREADS=8
    restart: unless-stopped
```

Then run: `docker-compose up -d`

</details>

## Platform Notes

- **Platform Dependency**: The server executable is platform-specific
  - Linux executable only runs on Linux
  - Windows executable (.exe) only runs on Windows
  - macOS executable only runs on macOS
- **For Cloud Deployment**: Use the Linux version or Docker
- **For Local Development**: Use the version matching your OS or Docker


## üá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è

<details>
<summary><strong>–ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ</strong></summary>

# üöÄ LLama Server - –ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π

<p align="center">
  <img src="assets/llama-service-logo.png" alt="–õ–æ–≥–æ—Ç–∏–ø Llama Service" width="320" />
</p>

<p align="center">
  <a href="#-–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"><img alt="–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã" src="https://img.shields.io/badge/platforms-Windows%20|%20Linux%20|%20macOS-1f6feb" /></a>
  <a href="#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-docker"><img alt="Docker" src="https://img.shields.io/badge/docker-ready-0db7ed" /></a>
  <a href="#-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ--–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-api"><img alt="API" src="https://img.shields.io/badge/API-OpenAI%20—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π-28a745" /></a>
  <a href="#-—Ä—É—á–Ω–∞—è-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–µ—Å–ª–∏-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–Ω–µ-—Å—Ä–∞–±–æ—Ç–∞–ª–∞"><img alt="Fallback" src="https://img.shields.io/badge/setup-–∞–≤—Ç–æ%20%2B%20—Ä—É—á–Ω–æ–π-6f42c1" /></a>
</p>

**–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLama —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ–¥–∏–Ω –∫–ª–∏–∫ –Ω–∞ –ª—é–±–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ!**

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è LLama —Å–µ—Ä–≤–µ—Ä–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∏ –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π.

---

## üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [‚ö° –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#-—Å—É–ø–µ—Ä-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [üîß –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –µ—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞](#-—Ä—É—á–Ω–∞—è-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–µ—Å–ª–∏-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–Ω–µ-—Å—Ä–∞–±–æ—Ç–∞–ª–∞)
- [üñ•Ô∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã](#-–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã)
- [üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞](#-—á—Ç–æ-–¥–µ–ª–∞–µ—Ç-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
- [üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏](#-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–∫–∞—Ç–∞–ª–æ–≥–æ–≤-–ø–æ—Å–ª–µ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
- [üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏](#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-–ø–æ—Å–ª–µ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
- [üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API](#-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ--–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-api)
- [üîß –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏](#-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
- [üõ†Ô∏è –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#-—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
- [üÜò –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫](#-—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ-–Ω–µ–ø–æ–ª–∞–¥–æ–∫)
- [üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ](#-–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è--–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ)
- [üê≥ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Docker](#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-docker)

## ‚ö° –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í—Å–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
1) –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å GGUF –≤ `models/` (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Phi-4-mini):
   - –ò—Å—Ç–æ—á–Ω–∏–∫: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
   - –ò–º—è —Ñ–∞–π–ª–∞: `models/phi-4-mini-instruct-q4_k_m.gguf`

2) –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏:
```bash
python main.py
```

3) –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä —Å –ø–æ–º–æ—â—å—é —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ (`start.bat` –Ω–∞ Windows –∏–ª–∏ `./start.sh` –Ω–∞ Linux/macOS).

–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (requests)
- ‚úÖ –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞—à—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
- ‚úÖ –°–æ–∑–¥–∞–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ï—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≤–∞—à–µ–π –û–°/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–º. —à–∞–≥–∏ —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∏–∂–µ.

## üîß –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –µ—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞

–ï—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ llama —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –≤–∞—à–µ–π –û–°/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–ª–µ–¥—É–π—Ç–µ —ç—Ç–∏–º —à–∞–≥–∞–º:

1) –°–∫–∞—á–∞–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å–±–æ—Ä–∫—É llama.cpp —Å–µ—Ä–≤–µ—Ä–∞ —Å: https://github.com/ggml-org/llama.cpp/releases
- Windows x64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server.exe` –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–µ–ª–∏–∑–∞ llama.cpp
- Linux x64/ARM64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server`
- macOS Intel/ARM64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server`

2) –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∞—Ä—Ö–∏–≤ –ª–æ–∫–∞–ª—å–Ω–æ –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
- –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ –∫–∞–∫:
  - Windows: `server.exe`
  - Linux/macOS: `server`
- –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ä—è–¥–æ–º —Å –Ω–∏–º (DLL –Ω–∞ Windows, `.so` –Ω–∞ Linux, `.dylib` –Ω–∞ macOS)

3) –°–æ–∑–¥–∞–π—Ç–µ –∫–∞—Ç–∞–ª–æ–≥ models –∏ –¥–æ–±–∞–≤—å—Ç–µ –º–æ–¥–µ–ª—å GGUF
```bash
mkdir -p models
# –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å:
# https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
# –ü–æ–º–µ—Å—Ç–∏—Ç–µ –∫–∞–∫: models/phi-4-mini-instruct-q4_k_m.gguf
```

4) –°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞
- Windows `start.bat`:
```bat
@echo off
set MODEL_PATH=models\phi-4-mini-instruct-q4_k_m.gguf
set HOST=0.0.0.0
set PORT=8080
set CONTEXT_SIZE=4096
set THREADS=8
set GPU_LAYERS=0

server.exe ^
  -m "%MODEL_PATH%" ^
  --host "%HOST%" ^
  --port "%PORT%" ^
  -c "%CONTEXT_SIZE%" ^
  -t "%THREADS%" ^
  --n-gpu-layers "%GPU_LAYERS%"
```

- Linux/macOS `start.sh`:
```bash
#!/usr/bin/env bash
set -euo pipefail
MODEL_PATH="models/phi-4-mini-instruct-q4_k_m.gguf"
HOST="0.0.0.0"
PORT="8080"
CONTEXT_SIZE="4096"
THREADS="8"
GPU_LAYERS="0"

chmod +x ./server
./server \
  -m "$MODEL_PATH" \
  --host "$HOST" \
  --port "$PORT" \
  -c "$CONTEXT_SIZE" \
  -t "$THREADS" \
  --n-gpu-layers "$GPU_LAYERS"
```

5) –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ
```bash
# Windows
start.bat

# Linux/macOS
./start.sh

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
curl http://localhost:8080/health
```

## üñ•Ô∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã

| –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –°—Ç–∞—Ç—É—Å |
|-----------|-------------|---------|
| Windows | x64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| Linux | x64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| Linux | ARM64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| macOS | Intel (x64) | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| macOS | Apple Silicon (ARM64) | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |

## üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞

1. ‚úÖ **–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞—à—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É** (Windows/Linux/macOS, x64/ARM64)
2. ‚úÖ **–°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Å–µ—Ä–≤–µ—Ä–∞** (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã)
3. ‚úÖ **–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** (DLL, –æ–±—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)
4. ‚úÖ **–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞** (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã)
5. ‚úÖ **–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã** (—Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API)
6. ‚úÖ **–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–∞–ª–æ–≥–æ–≤** (–ø–∞–ø–∫–∞ models, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

```
–≤–∞—à-–ø—Ä–æ–µ–∫—Ç/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ phi-4-mini-instruct-q4_k_m.gguf  # AI –º–æ–¥–µ–ª—å (2.2GB)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ server.exe (–∏–ª–∏ server)               # –û—Å–Ω–æ–≤–Ω–æ–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ *.dll (–∏–ª–∏ *.so/*.dylib)             # –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
‚îú‚îÄ‚îÄ start.bat (–∏–ª–∏ start.sh)             # –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞
‚îú‚îÄ‚îÄ test_api.py                          # –°–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API
‚îú‚îÄ‚îÄ auto_setup_llama.py                  # –°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏

‚îî‚îÄ‚îÄ README.md                            # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

–°–º. —à–∞–≥–∏ –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ (—Å–∫–∞—á–∞—Ç—å GGUF ‚Üí –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫—É ‚Üí —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞).

- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ó–∞–ø—É—Å—Ç–∏—Ç–µ `python test_api.py` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
- –û—Ç–∫—Ä–æ–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ –∞–¥—Ä–µ—Å—É http://localhost:8080

## üéâ –ì–æ—Ç–æ–≤–æ!

–í–∞—à LLama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ `http://localhost:8080` —Å:
- ‚úÖ API —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Å OpenAI
- ‚úÖ –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏

## üìã –ß—Ç–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è

### Windows
- `server.exe` - –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª LLama —Å–µ—Ä–≤–µ—Ä–∞
- `ggml-base.dll` - –û—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ GGML
- `ggml-cpu-*.dll` - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ CPU
- `ggml.dll` - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å GGML
- `llama.dll` - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ LLama
- `libcurl-x64.dll` - HTTP –∫–ª–∏–µ–Ω—Ç
- `libomp140.x86_64.dll` - –°—Ä–µ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è OpenMP
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ DLL

### Linux/macOS
- `server` - –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª LLama —Å–µ—Ä–≤–µ—Ä–∞
- `*.so` / `*.dylib` - –û–±—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API

### –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
python test_api.py

# –ò–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é
curl http://localhost:8080/health
```

<details>
<summary><strong>–ü—Ä–∏–º–µ—Ä—ã API (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å)</strong></summary>

#### –ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
```bash
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
    "n_predict": 100,
    "temperature": 0.7
  }'
```

#### Chat —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}],
    "max_tokens": 50
  }'
```

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ü–æ–ª–µ "model" —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å OpenAI, –Ω–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å - —ç—Ç–æ –ª—é–±–æ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª GGUF (–Ω–∞–ø—Ä–∏–º–µ—Ä, `phi-4-mini-instruct-q4_k_m.gguf`).

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
```bash
curl http://localhost:8080/health
```

#### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–µ—Ä–µ
```bash
curl http://localhost:8080/v1/models
```

</details>

## üîß –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç Windows/Linux/macOS
- –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É x64/ARM64
- –°–∫–∞—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ –¥–ª—è –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã

### –ü–æ–ª–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
- –°–∫–∞—á–∏–≤–∞–µ—Ç –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã
- –ù–∏–∫–∞–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ DLL/–±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏
- –ì–æ—Ç–æ–≤–∞—è –∫ –∑–∞–ø—É—Å–∫—É –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–∫—Ä–∏–ø—Ç—ã
- –°–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
- –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
- –ß–µ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö –∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
- –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
- –ü–æ—à–∞–≥–æ–≤—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
- –ß–µ—Ç–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Å–ø–µ—Ö–∞/–Ω–µ—É–¥–∞—á–∏

### –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É
- **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –ù–∞—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è C++ llama.cpp
- **API —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI**: –ü—Ä—è–º–∞—è –∑–∞–º–µ–Ω–∞ –¥–ª—è —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ OpenAI
- **–ù–∏–∑–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏**: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
- **–ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫**: –ù–∏–∫–∞–∫–∏—Ö –∑–∞–¥–µ—Ä–∂–µ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏

## üõ†Ô∏è –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏
- –•–æ—Å—Ç–∞/–ø–æ—Ä—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞
- –†–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤
- GPU —Å–ª–æ–µ–≤

### –ù–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π
- –ü–æ–º–µ—Å—Ç–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ GGUF –≤ `models/`
- –û–±–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é MODEL_PATH –≤ —Å–∫—Ä–∏–ø—Ç–µ –∑–∞–ø—É—Å–∫–∞
- –ü–µ—Ä–µ–∫–ª—é—á–∞–π—Ç–µ—Å—å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

### –û–ø—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞

| –û–ø—Ü–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|-------|----------|--------------|
| `-m, --model` | –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ | –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ |
| `--host` | IP –∞–¥—Ä–µ—Å –¥–ª—è –ø—Ä–∏–≤—è–∑–∫–∏ | 0.0.0.0 |
| `--port` | –ü–æ—Ä—Ç –¥–ª—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è | 8080 |
| `-c, --ctx-size` | –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | 4096 |
| `-t, --threads` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ | 8 |
| `--n-gpu-layers` | GPU —Å–ª–æ–∏ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ | 0 |
| `--log-disable` | –û—Ç–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ | false |

## üÜò –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ü—Ä–æ–±–ª–µ–º—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- **Python –Ω–µ –Ω–∞–π–¥–µ–Ω**: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Python 3.7+ —Å python.org, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ "Add to PATH" –æ—Ç–º–µ—á–µ–Ω–æ
- **–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–µ—Ç—Å—è**: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Å–Ω–æ–≤–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è)
- **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ requests –Ω–µ —É–¥–∞–µ—Ç—Å—è**: –í—Ä—É—á–Ω—É—é –∑–∞–ø—É—Å—Ç–∏—Ç–µ `pip install requests` –∑–∞—Ç–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É

### –ü—Ä–æ–±–ª–µ–º—ã —Å–µ—Ä–≤–µ—Ä–∞  
- **–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞**: –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å GGUF –≤ –ø–∞–ø–∫—É `models/`
- **–ü–æ—Ä—Ç –∑–∞–Ω—è—Ç**: –ò–∑–º–µ–Ω–∏—Ç–µ PORT –≤ —Å–∫—Ä–∏–ø—Ç–µ –∑–∞–ø—É—Å–∫–∞
- **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- **–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç DLL**: –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

### –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
python test_api.py

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–∞
curl http://localhost:8080/health

# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
dir models\*.gguf    # Windows
ls models/*.gguf     # Linux/macOS

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
python main.py
```

## üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏
1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–Ω–æ–≤–∞: `python main.py`
2. –í—ã–±–µ—Ä–∏—Ç–µ 'y' –∫–æ–≥–¥–∞ —Å–ø—Ä–æ—Å—è—Ç –æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
3. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è

### –ß–∏—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞
1. –£–¥–∞–ª–∏—Ç–µ —Ñ–∞–π–ª—ã `server.exe` –∏ `*.dll`
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–≤–µ–∂–µ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏
3. –ú–æ–¥–µ–ª–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ `models/` —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è

## üöÄ –ß—Ç–æ –¥–∞–ª—å—à–µ?

- **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ API —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI –≤ –≤–∞—à–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö
- **–ù–∞—Å—Ç—Ä–æ–π–∫–∞**: –†–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞ –ø–æ–¥ –≤–∞—à–∏ –Ω—É–∂–¥—ã  
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ**: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–π—Ç–µ –≤ –æ–±–ª–∞–∫–µ –∏–ª–∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤
- **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ**: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å Hugging Face

## üìö –†–µ—Å—É—Ä—Å—ã –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞

- **–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ `models/README.md` –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
- **–ü—Ä–æ–±–ª–µ–º—ã**: –°–æ–æ–±—â–∞–π—Ç–µ –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —á–µ—Ä–µ–∑ GitHub Issues
- **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `python test_api.py` –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API**: –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å OpenAI –Ω–∞ `/v1/chat/completions`
- **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è**: –†–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫

## üê≥ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Docker

<details>
<summary><strong>–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Docker (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å)</strong></summary>

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å Docker

1. **–°–æ–±–µ—Ä–∏—Ç–µ –æ–±—Ä–∞–∑:**
```bash
docker build -t llama-server .
```

2. **–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å** –≤ –≤–∞—à –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ `models/` —Å–Ω–∞—á–∞–ª–∞:
```bash
# –ü—Ä–∏–º–µ—Ä: –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å Phi-4-mini (Q4_K_M) —Å Hugging Face
# –ü–æ–º–µ—Å—Ç–∏—Ç–µ phi-4-mini-instruct-q4_k_m.gguf –≤ ./models/
# –ò—Å—Ç–æ—á–Ω–∏–∫: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
```

3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä:**
```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  llama-server
```

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Docker

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–µ—Ä–≤–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è:

```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  -e MODEL_PATH="models/your-model.gguf" \
  -e CONTEXT_SIZE="8192" \
  -e THREADS="16" \
  llama-server
```

### –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|------------|----------|--------------|
| `MODEL_PATH` | –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ GGUF | `models/phi-4-mini-instruct-q4_k_m.gguf` |
| `HOST` | –•–æ—Å—Ç –¥–ª—è –ø—Ä–∏–≤—è–∑–∫–∏ | `0.0.0.0` |
| `PORT` | –ü–æ—Ä—Ç –¥–ª—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è | `8080` |
| `CONTEXT_SIZE` | –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | `4096` |
| `THREADS` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ | `8` |
| `GPU_LAYERS` | GPU —Å–ª–æ–∏ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ | `0` |

### Docker Compose (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–°–æ–∑–¥–∞–π—Ç–µ `docker-compose.yml`:

```yaml
version: '3.8'
services:
  llama-server:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_PATH=models/phi-4-mini-instruct-q4_k_m.gguf
      - CONTEXT_SIZE=4096
      - THREADS=8
    restart: unless-stopped
```

–ó–∞—Ç–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç–µ: `docker-compose up -d`

</details>

## –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö

- **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã**: –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Å–µ—Ä–≤–µ—Ä–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω –¥–ª—è –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
  - Linux –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ Linux
  - Windows –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª (.exe) —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ Windows
  - macOS –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ macOS
- **–î–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ—Ä—Å–∏—é Linux –∏–ª–∏ Docker
- **–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ—Ä—Å–∏—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–∞—à–µ–π –û–° –∏–ª–∏ Docker

</details>
