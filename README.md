# üöÄ LLama Server - Complete Auto-Setup Solution

<p align="center">
  <img src="assets/llama-service-logo.png" alt="Llama Service Logo" width="320" />
</p>

<p align="center">
  <a href="#-supported-platforms"><img alt="Platforms" src="https://img.shields.io/badge/platforms-Windows%20|%20Linux%20|%20macOS-1f6feb" /></a>
  <a href="#-docker-usage"><img alt="Docker" src="https://img.shields.io/badge/docker-ready-0db7ed" /></a>
  <a href="#-testing--api-usage"><img alt="API" src="https://img.shields.io/badge/API-OpenAI%20compatible-28a745" /></a>
  <a href="#-manual-setup-if-auto-setup-fails"><img alt="Fallback" src="https://img.shields.io/badge/setup-auto%20%2B%20manual-6f42c1" /></a>
</p>

**One-click setup for LLama server on any platform!**

 LLama server implementation with automatic setup, dependency management, and cross-platform support.

---

## üìö Table of Contents

- [‚ö° Super Quick Start](#-super-quick-start)
- [üîß Manual setup if auto-setup fails](#-manual-setup-if-auto-setup-fails)
- [üñ•Ô∏è Supported Platforms](#-supported-platforms)
- [üìã What Auto-Setup Does](#-what-auto-setup-does)
- [üìÅ Directory Structure After Setup](#-directory-structure-after-setup)
- [üéØ Usage After Setup](#-usage-after-setup)
- [üß™ Testing & API Usage](#-testing--api-usage)
- [üîß Features](#-features)
- [üõ†Ô∏è Advanced Configuration](#-advanced-configuration)
- [üÜò Troubleshooting](#-troubleshooting)
- [üîÑ Updates & Maintenance](#-updates--maintenance)
- [üê≥ Docker Usage](#-docker-usage)
- [üá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è](#-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç-—Ä—É—Å—Å–∫–∏–π)

## ‚ö° Super Quick Start

### All Platforms
1) Download a GGUF model to `models/` (recommended Phi-4-mini):
   - Source: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
   - File name: `models/phi-4-mini-instruct-q4_k_m.gguf`

2) Run the auto-setup script:
```bash
python main.py
```

**Note**: Depending on your system setup, you may need to use:
- `python3 main.py` (Linux/macOS, or if Python 3 is installed as python3)
- `py main.py` (Windows, if using Python launcher)
- `python main.py` (Windows, if Python is in PATH)

3) Start the server using the generated script (`start.bat` on Windows or `./start.sh` on Linux/macOS).

The auto-setup handles the server binary and dependencies automatically:
- ‚úÖ Auto-installs its own dependencies (requests)
- ‚úÖ Detects your platform and downloads correct files
- ‚úÖ Creates platform-specific startup scripts

Note: If auto-setup fails to fetch the correct server for your OS/arch, see the manual setup steps below.

## üîß Manual setup if auto-setup fails

If the auto-setup script can't fetch the correct llama server binary for your OS/arch, follow these steps:

1) Download the correct llama.cpp server build from: https://github.com/ggml-org/llama.cpp/releases
- Windows x64: grab the zip containing `llama-server.exe` from the latest llama.cpp release
- Linux x64/ARM64: grab the zip containing `llama-server`
- macOS Intel/ARM64: grab the zip containing `llama-server`

2) Extract the archive locally and copy files into the project root
- Place the server binary as:
  - Windows: `server.exe`
-  Linux/macOS: `server`
- Copy all required libraries next to it (DLLs on Windows, `.so` on Linux, `.dylib` on macOS)

3) Create the models directory and add a GGUF model
```bash
mkdir -p models
# Download the model:
# https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
# Place as: models/phi-4-mini-instruct-q4_k_m.gguf
```

4) Create a startup script
- Windows `start.bat`:
```bat
@echo off
set MODEL_PATH=models\phi-4-mini-instruct-q4_k_m.gguf
set HOST=0.0.0.0
set PORT=8080
set CONTEXT_SIZE=4096
set THREADS=8
set GPU_LAYERS=0

server.exe ^
  -m "%MODEL_PATH%" ^
  --host "%HOST%" ^
  --port "%PORT%" ^
  -c "%CONTEXT_SIZE%" ^
  -t "%THREADS%" ^
  --n-gpu-layers "%GPU_LAYERS%"
```

- Linux/macOS `start.sh`:
```bash
#!/bin/bash
set -euo pipefail
MODEL_PATH="models/phi-4-mini-instruct-q4_k_m.gguf"
HOST="0.0.0.0"
PORT="8080"
CONTEXT_SIZE="4096"
THREADS="8"
GPU_LAYERS="0"

chmod +x ./server
./server \
  -m "$MODEL_PATH" \
  --host "$HOST" \
  --port "$PORT" \
  -c "$CONTEXT_SIZE" \
  -t "$THREADS" \
  --n-gpu-layers "$GPU_LAYERS"
```

5) Start and test
```bash
# Windows
start.bat

# Linux/macOS
./start.sh

# Health
curl http://localhost:8080/health
```

## üñ•Ô∏è Supported Platforms

| Platform | Architecture | Status |
|----------|-------------|---------|
| Windows | x64 | ‚úÖ Fully Supported |
| Linux | x64 | ‚úÖ Fully Supported |
| Linux | ARM64 | ‚úÖ Fully Supported |
| macOS | Intel (x64) | ‚úÖ Fully Supported |
| macOS | Apple Silicon (ARM64) | ‚úÖ Fully Supported |

## üìã What Auto-Setup Does

1. ‚úÖ **Detects your platform** (Windows/Linux/macOS, x64/ARM64)
2. ‚úÖ **Downloads server executable** (correct version for your system)
3. ‚úÖ **Downloads all dependencies** (DLLs, shared libraries)
4. ‚úÖ **Creates startup scripts** (platform-optimized)
5. ‚úÖ **Creates test utilities** (API testing script)
6. ‚úÖ **Sets up directory structure** (models folder, documentation)

## üìÅ Directory Structure

```
your-project/
‚îú‚îÄ‚îÄ main.py                               # Entry point script
‚îú‚îÄ‚îÄ setup.py                              # Core setup class
‚îú‚îÄ‚îÄ platform_detector.py                  # Platform detection module
‚îú‚îÄ‚îÄ downloader.py                         # Download utilities
‚îú‚îÄ‚îÄ extractor.py                          # File extraction utilities
‚îú‚îÄ‚îÄ file_creator.py                       # Startup/test scripts creator
‚îú‚îÄ‚îÄ auto_setup_llama.py                   # Legacy setup script
‚îú‚îÄ‚îÄ models/                               # Model files directory
‚îÇ   ‚îú‚îÄ‚îÄ phi-4-mini-instruct-q4_k_m.gguf  # AI model (2.2GB)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        # Model download guide
‚îú‚îÄ‚îÄ server.exe (or server)               # Main executable (after setup)
‚îú‚îÄ‚îÄ *.dll (or *.so/*.dylib)             # Required libraries (after setup)
‚îú‚îÄ‚îÄ start.bat (or start.sh)             # Startup script (after setup)
‚îú‚îÄ‚îÄ test_api.py                          # API test script (after setup)
‚îú‚îÄ‚îÄ Dockerfile                            # Docker configuration
‚îî‚îÄ‚îÄ README.md                            # This file
```

## üéØ Usage After Setup

Refer to Super Quick Start steps (download GGUF ‚Üí run auto-setup ‚Üí start script).

- Optional: Run `python test_api.py` to verify endpoints
- Open the Web UI at http://localhost:8080

## üéâ You're Done!

Your LLama server is running at `http://localhost:8080` with:
- ‚úÖ OpenAI-compatible API endpoints
- ‚úÖ Web interface for testing
- ‚úÖ All dependencies properly configured

## üìã What Gets Downloaded

### Windows
- `server.exe` - LLama server executable
- `ggml-base.dll` - Core GGML library
- `ggml-cpu-*.dll` - CPU optimization libraries
- `ggml.dll` - GGML interface
- `llama.dll` - LLama library
- `libcurl-x64.dll` - HTTP client
- `libomp140.x86_64.dll` - OpenMP runtime
- Additional support DLLs

### Linux/macOS
- `server` - LLama server executable
- `*.so` / `*.dylib` - Shared libraries
- All required dependencies

 

## üß™ Testing & API Usage

### Test Everything Works
```bash
# Run the comprehensive test script
python test_api.py

# Or check manually
curl http://localhost:8080/health
```

**Note**: Use the appropriate Python command for your system:
- `python3 test_api.py` (Linux/macOS, or if Python 3 is installed as python3)
- `py test_api.py` (Windows, if using Python launcher)
- `python test_api.py` (Windows, if Python is in PATH)

<details>
<summary><strong>API Examples (expand)</strong></summary>

#### Simple Completion
```bash
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is artificial intelligence?",
    "n_predict": 100,
    "temperature": 0.7
  }'
```

#### OpenAI-Compatible Chat
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 50
  }'
```

Note: The "model" field is required for OpenAI compatibility but the actual model used is whatever GGUF file you loaded (e.g., `phi-4-mini-instruct-q4_k_m.gguf`).

#### Health Check
```bash
curl http://localhost:8080/health
```

#### Server Info
```bash
curl http://localhost:8080/v1/models
```

</details>



## üîß Features

### Smart Platform Detection
- Automatically detects Windows/Linux/macOS
- Identifies x64/ARM64 architecture
- Downloads correct binaries for your system

### Complete Dependency Management
- Downloads ALL required files
- No missing DLL/library issues
- Ready-to-run setup

### User-Friendly Scripts
- Platform-appropriate startup scripts
- Comprehensive test scripts
- Clear error messages and guidance

### Progress Tracking
- Download progress bars
- Step-by-step status updates
- Clear success/failure indicators

### Production Ready
- **High performance**: Native C++ llama.cpp implementation
- **OpenAI-compatible API**: Drop-in replacement for OpenAI endpoints
- **Low memory footprint**: Minimal resource usage
- **Fast startup**: No model loading delays

## üõ†Ô∏è Advanced Configuration

### Custom Configuration
Edit the generated startup script to modify:
- Model path
- Server host/port
- Context size
- Thread count
- GPU layers

### Multiple Models
- Place multiple GGUF files in `models/`
- Update startup script MODEL_PATH variable
- Switch between models as needed

### Server Configuration Options

| Option | Description | Default |
|--------|-------------|---------|
| `-m, --model` | Path to model file | Required |
| `--host` | IP address to bind to | 0.0.0.0 |
| `--port` | Port to listen on | 8080 |
| `-c, --ctx-size` | Context window size | 4096 |
| `-t, --threads` | Number of threads | 8 |
| `--n-gpu-layers` | GPU layers to offload | 0 |
| `--log-disable` | Disable logging | false |

## üÜò Troubleshooting

### Setup Problems
- **Python not found**: Install Python 3.7+ from python.org, ensure "Add to PATH" is checked
- **Python command not working**: Try `python3`, `py`, or `python` depending on your system setup
- **Download fails**: Check internet connection, try running setup again (auto-resumes)
- **Requests install fails**: Manually run `pip install requests` then retry setup

### Server Problems  
- **Model not found**: Download a GGUF model to `models/` folder
- **Port busy**: Change PORT in startup script
- **Out of memory**: Use smaller model or reduce context size
- **Missing DLLs**: Re-run auto-setup to download all dependencies

### Quick Diagnostics
```bash
# Test if everything works
python test_api.py

# Check server health
curl http://localhost:8080/health

# Verify model exists
dir models\*.gguf    # Windows
ls models/*.gguf     # Linux/macOS

# Re-run setup if something went wrong
python main.py
```

## üîÑ Updates & Maintenance

### Update to Latest Version
1. Run the setup script again: `python main.py`
2. Choose 'y' when asked to overwrite existing files
3. Existing models and custom configs are preserved

### Clean Installation
1. Delete `server.exe` and `*.dll` files
2. Run setup script for fresh installation
3. Models in `models/` directory are kept

## üöÄ What's Next?

- **Integrate**: Use OpenAI-compatible API in your apps
- **Customize**: Edit startup scripts for your needs  
- **Scale**: Deploy to cloud or run multiple instances
- **Explore**: Try different models from Hugging Face

## üìö Resources & Support

- **Model Downloads**: Check `models/README.md` for recommended models and download links
- **Issues**: Report problems via GitHub Issues
- **Testing**: Use `python test_api.py` for comprehensive diagnostics
- **API Documentation**: OpenAI-compatible endpoints at `/v1/chat/completions`
- **Configuration**: Edit startup scripts for custom settings

## üê≥ Docker Usage

<details>
<summary><strong>Docker quick start and configuration (expand)</strong></summary>

### Quick Start with Docker

1. **Build the image:**
```bash
docker build -t llama-server .
```

2. **Download a model** to your local `models/` directory first:
```bash
# Example: Download Phi-4-mini model (Q4_K_M) from Hugging Face
# Place phi-4-mini-instruct-q4_k_m.gguf in ./models/
# Source: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
```

3. **Run the container:**
```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  llama-server
```

### Docker Environment Variables

Customize the server by setting environment variables:

```bash
docker run -d \
  --name llama-server \
  -p 8080:8080 \
  -v $(pwd)/models:/app/models \
  -e MODEL_PATH="models/your-model.gguf" \
  -e CONTEXT_SIZE="8192" \
  -e THREADS="16" \
  llama-server
```

### Available Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `MODEL_PATH` | Path to GGUF model file | `models/phi-4-mini-instruct-q4_k_m.gguf` |
| `HOST` | Host to bind to | `0.0.0.0` |
| `PORT` | Port to listen on | `8080` |
| `CONTEXT_SIZE` | Context window size | `4096` |
| `THREADS` | Number of threads | `8` |
| `GPU_LAYERS` | GPU layers to offload | `0` |

### Docker Compose (Optional)

Create a `docker-compose.yml`:

```yaml
version: '3.8'
services:
  llama-server:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_PATH=models/phi-4-mini-instruct-q4_k_m.gguf
      - CONTEXT_SIZE=4096
      - THREADS=8
    restart: unless-stopped
```

Then run: `docker-compose up -d`

</details>

## Platform Notes

- **Platform Dependency**: The server executable is platform-specific
  - Linux executable only runs on Linux
  - Windows executable (.exe) only runs on Windows
  - macOS executable only runs on macOS
- **For Cloud Deployment**: Use the Linux version or Docker
- **For Local Development**: Use the version matching your OS or Docker


## üá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è

<details>
<summary><strong>–ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ</strong></summary>

# üöÄ LLama Server - –ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π

<p align="center">
  <img src="assets/llama-service-logo.png" alt="–õ–æ–≥–æ—Ç–∏–ø Llama Service" width="320" />
</p>

<p align="center">
  <a href="#-–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"><img alt="–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã" src="https://img.shields.io/badge/platforms-Windows%20|%20Linux%20|%20macOS-1f6feb" /></a>
  <a href="#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-docker"><img alt="Docker" src="https://img.shields.io/badge/docker-ready-0db7ed" /></a>
  <a href="#-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ--–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-api"><img alt="API" src="https://img.shields.io/badge/API-OpenAI%20—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π-28a745" /></a>
  <a href="#-—Ä—É—á–Ω–∞—è-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–µ—Å–ª–∏-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–Ω–µ-—Å—Ä–∞–±–æ—Ç–∞–ª–∞"><img alt="Fallback" src="https://img.shields.io/badge/setup-–∞–≤—Ç–æ%20%2B%20—Ä—É—á–Ω–æ–π-6f42c1" /></a>
</p>

**–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLama —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ–¥–∏–Ω –∫–ª–∏–∫ –Ω–∞ –ª—é–±–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ!**

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è LLama —Å–µ—Ä–≤–µ—Ä–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∏ –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π.

---

## üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [‚ö° –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#-—Å—É–ø–µ—Ä-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [üîß –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –µ—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞](#-—Ä—É—á–Ω–∞—è-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–µ—Å–ª–∏-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–Ω–µ-—Å—Ä–∞–±–æ—Ç–∞–ª–∞)
- [üñ•Ô∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã](#-–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã)
- [üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞](#-—á—Ç–æ-–¥–µ–ª–∞–µ—Ç-–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
- [üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏](#-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–∫–∞—Ç–∞–ª–æ–≥–æ–≤-–ø–æ—Å–ª–µ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
- [üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏](#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-–ø–æ—Å–ª–µ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
- [üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API](#-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ--–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-api)
- [üîß –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏](#-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
- [üõ†Ô∏è –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#-—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
- [üÜò –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫](#-—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ-–Ω–µ–ø–æ–ª–∞–¥–æ–∫)
- [üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ](#-–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è--–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ)
- [üê≥ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Docker](#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-docker)

## ‚ö° –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í—Å–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
1) –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å GGUF –≤ `models/` (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Phi-4-mini):
   - –ò—Å—Ç–æ—á–Ω–∏–∫: https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
   - –ò–º—è —Ñ–∞–π–ª–∞: `models/phi-4-mini-instruct-q4_k_m.gguf`

2) –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏:
```bash
python main.py
```

3) –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä —Å –ø–æ–º–æ—â—å—é —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ (`start.bat` –Ω–∞ Windows –∏–ª–∏ `./start.sh` –Ω–∞ Linux/macOS).

–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (requests)
- ‚úÖ –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞—à—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
- ‚úÖ –°–æ–∑–¥–∞–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ï—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≤–∞—à–µ–π –û–°/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–º. —à–∞–≥–∏ —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∏–∂–µ.

## üîß –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –µ—Å–ª–∏ –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞

–ï—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ llama —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –≤–∞—à–µ–π –û–°/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–ª–µ–¥—É–π—Ç–µ —ç—Ç–∏–º —à–∞–≥–∞–º:

1) –°–∫–∞—á–∞–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å–±–æ—Ä–∫—É llama.cpp —Å–µ—Ä–≤–µ—Ä–∞ —Å: https://github.com/ggml-org/llama.cpp/releases
- Windows x64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server.exe` –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–µ–ª–∏–∑–∞ llama.cpp
- Linux x64/ARM64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server`
- macOS Intel/ARM64: –≤–æ–∑—å–º–∏—Ç–µ zip —Å–æ–¥–µ—Ä–∂–∞—â–∏–π `llama-server`

2) –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∞—Ä—Ö–∏–≤ –ª–æ–∫–∞–ª—å–Ω–æ –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
- –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä–Ω—ã–π –±–∏–Ω–∞—Ä–Ω–∏–∫ –∫–∞–∫:
  - Windows: `server.exe`
  - Linux/macOS: `server`
- –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ä—è–¥–æ–º —Å –Ω–∏–º (DLL –Ω–∞ Windows, `.so` –Ω–∞ Linux, `.dylib` –Ω–∞ macOS)

3) –°–æ–∑–¥–∞–π—Ç–µ –∫–∞—Ç–∞–ª–æ–≥ models –∏ –¥–æ–±–∞–≤—å—Ç–µ –º–æ–¥–µ–ª—å GGUF
```bash
mkdir -p models
# –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å:
# https://huggingface.co/matrixportalx/Phi-4-mini-instruct-Q4_K_M-GGUF
# –ü–æ–º–µ—Å—Ç–∏—Ç–µ –∫–∞–∫: models/phi-4-mini-instruct-q4_k_m.gguf
```

4) –°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞
- Windows `start.bat`:
```bat
@echo off
set MODEL_PATH=models\phi-4-mini-instruct-q4_k_m.gguf
set HOST=0.0.0.0
set PORT=8080
set CONTEXT_SIZE=4096
set THREADS=8
set GPU_LAYERS=0

server.exe ^
  -m "%MODEL_PATH%" ^
  --host "%HOST%" ^
  --port "%PORT%" ^
  -c "%CONTEXT_SIZE%" ^
  -t "%THREADS%" ^
  --n-gpu-layers "%GPU_LAYERS%"
```

- Linux/macOS `start.sh`:
```bash
#!/bin/bash
set -euo pipefail
MODEL_PATH="models/phi-4-mini-instruct-q4_k_m.gguf"
HOST="0.0.0.0"
PORT="8080"
CONTEXT_SIZE="4096"
THREADS="8"
GPU_LAYERS="0"

chmod +x ./server
./server \
  -m "$MODEL_PATH" \
  --host "$HOST" \
  --port "$PORT" \
  -c "$CONTEXT_SIZE" \
  -t "$THREADS" \
  --n-gpu-layers "$GPU_LAYERS"
```

5) –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ
```bash
# Windows
start.bat

# Linux/macOS
./start.sh

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
curl http://localhost:8080/health
```

## üñ•Ô∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã

| –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –°—Ç–∞—Ç—É—Å |
|-----------|-------------|---------|
| Windows | x64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| Linux | x64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| Linux | ARM64 | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| macOS | Intel (x64) | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |
| macOS | Apple Silicon (ARM64) | ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è |

## üìã –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞

1. ‚úÖ **–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞—à—É –ø–ª–∞—Ç—Ñ–æ—Ä–º—É** (Windows/Linux/macOS, x64/ARM64)
2. ‚úÖ **–°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Å–µ—Ä–≤–µ—Ä–∞** (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã)
3. ‚úÖ **–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** (DLL, –æ–±—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)
4. ‚úÖ **–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞** (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã)
5. ‚úÖ **–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã** (—Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API)
6. ‚úÖ **–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–∞–ª–æ–≥–æ–≤** (–ø–∞–ø–∫–∞ models, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

```
–≤–∞—à-–ø—Ä–æ–µ–∫—Ç/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ phi-4-mini-instruct-q4_k_m.gguf  # AI –º–æ–¥–µ–ª—å (2.2GB)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ server.exe (–∏–ª–∏ server)               # –û—Å–Ω–æ–≤–Ω–æ–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ *.dll (–∏–ª–∏ *.so/*.dylib)             # –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
‚îú‚îÄ‚îÄ start.bat (–∏–ª–∏ start.sh)             # –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞
‚îú‚îÄ‚îÄ test_api.py                          # –°–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API
‚îú‚îÄ‚îÄ auto_setup_llama.py                  # –°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏

‚îî‚îÄ‚îÄ README.md                            # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

–°–º. —à–∞–≥–∏ –°—É–ø–µ—Ä –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ (—Å–∫–∞—á–∞—Ç—å GGUF ‚Üí –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫—É ‚Üí —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞).

- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ó–∞–ø—É—Å—Ç–∏—Ç–µ `python test_api.py` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
- –û—Ç–∫—Ä–æ–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ –∞–¥—Ä–µ—Å—É http://localhost:8080

## üéâ –ì–æ—Ç–æ–≤–æ!

–í–∞—à LLama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ `http://localhost:8080` —Å:
- ‚úÖ API —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Å OpenAI
- ‚úÖ –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏

## üìã –ß—Ç–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è

### Windows
- `server.exe` - –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª LLama —Å–µ—Ä–≤–µ—Ä–∞
- `ggml-base.dll` - –û—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ GGML
- `ggml-cpu-*.dll` - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ CPU
- `ggml.dll` - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å GGML
- `llama.dll` - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ LLama
- `libcurl-x64.dll` - HTTP –∫–ª–∏–µ–Ω—Ç
- `libomp140.x86_64.dll` - –°—Ä–µ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è OpenMP
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ DLL

### Linux/macOS
- `server` - –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª LLama —Å–µ—Ä–≤–µ—Ä–∞
- `*.so` / `*.dylib` - –û–±—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API

### –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
python test_api.py

# –ò–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é
curl http://localhost:8080/health
```

<details>
<summary><strong>–ü—Ä–∏–º–µ—Ä—ã API (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å)</strong></summary>

#### –ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
```bash
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
    "n_predict": 100,
    "temperature": 0.7
  }'
```

#### Chat —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI
```bash
curl -X POST http://localhost:8080/v1/chat/completions
